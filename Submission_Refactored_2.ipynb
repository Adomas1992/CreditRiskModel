{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = datetime(1970, 1, 1)\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(None)\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_0_0_path, static_0_1_path, static_cb_0_path, schema_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_0_0_path = static_0_0_path\n",
    "        self.static_0_1_path = static_0_1_path\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = {}\n",
    "\n",
    "    def load_data(self, path):\n",
    "        try:\n",
    "            df = pl.read_parquet(path)\n",
    "            self.update_schema(df)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_schema(self, dataframe):\n",
    "        for col, dtype in zip(dataframe.columns, dataframe.dtypes):\n",
    "            if col not in self.global_schema:\n",
    "                self.global_schema[col] = str(dtype)\n",
    "\n",
    "    def save_schema(self):\n",
    "        with open(self.schema_path, 'w') as file:\n",
    "            json.dump(self.global_schema, file)\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "        return data.select([col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns])\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        categorical_columns = ['education_1103M', 'maritalst_385M']\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data = data.with_columns(data[col].cast(pl.Categorical))\n",
    "        columns_to_drop = [col for col in data.columns if data[col].dtype == pl.Utf8 and col not in date_columns and col not in categorical_columns]\n",
    "        data = data.drop(columns_to_drop)\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, data_base, data_static_0_0, data_static_0_1, data_static_cb_0):\n",
    "        concatenated_data = pl.concat([data_static_0_0, data_static_0_1], how='vertical')\n",
    "        merged_data = data_base.join(concatenated_data, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        data_static_0_0 = self.load_data(self.static_0_0_path)\n",
    "        data_static_0_1 = self.load_data(self.static_0_1_path)\n",
    "        data_static_cb_0 = self.load_data(self.static_cb_0_path)\n",
    "        self.save_schema()\n",
    "        data_base = self.preprocess_base(data_base)\n",
    "        data_static_0_0 = self.preprocess_static(data_static_0_0)\n",
    "        data_static_0_1 = self.preprocess_static(data_static_0_1)\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(data_static_cb_0)\n",
    "        return self.merge_data(data_base, data_static_0_0, data_static_0_1, data_static_cb_0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schema_path = \"C:/Users/afise/.git/CreditRiskModel/unified_schema.json\"\n",
    "    pipeline = DataPipeline_Depth_0(\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_base.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_cb_0.parquet\",\n",
    "        schema_path\n",
    "    )\n",
    "    Depth_0 = pipeline.execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = datetime(1970, 1, 1)\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(None)\n",
    "\n",
    "def dtype_mapping(dtype_str):\n",
    "    mapping = {\n",
    "        'Int32': pl.Int32,\n",
    "        'Int64': pl.Int64,\n",
    "        'Float32': pl.Float32,\n",
    "        'Float64': pl.Float64,\n",
    "        'Utf8': pl.Utf8,\n",
    "        'Boolean': pl.Boolean,\n",
    "        'Date': pl.Date,\n",
    "        'Categorical': pl.Categorical\n",
    "    }\n",
    "    return mapping.get(dtype_str, pl.Utf8)\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_paths, static_cb_0_path, schema_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_paths = static_paths\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = self.load_schema()\n",
    "\n",
    "    def load_schema(self):\n",
    "        with open(self.schema_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        df = pl.read_parquet(path)\n",
    "        return self.ensure_schema(df)\n",
    "\n",
    "    def ensure_schema(self, dataframe):\n",
    "        for col, expected_dtype in self.global_schema.items():\n",
    "            expected_pl_dtype = dtype_mapping(expected_dtype)\n",
    "            if col in dataframe.columns:\n",
    "                if dataframe[col].dtype != expected_pl_dtype:\n",
    "                    dataframe = dataframe.with_columns(dataframe[col].cast(expected_pl_dtype))\n",
    "            else:\n",
    "                dataframe = dataframe.with_columns(pl.lit(None).cast(expected_pl_dtype))\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "        columns_to_keep = [col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns]\n",
    "        data = data.select(columns_to_keep)\n",
    "        return data\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        categorical_columns = ['education_1103M', 'maritalst_385M']\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data = data.with_columns(data[col].cast(pl.Categorical))\n",
    "        columns_to_drop = [col for col in data.columns if data[col].dtype == pl.Utf8 and col not in date_columns and col not in categorical_columns]\n",
    "        data = data.drop(columns_to_drop)\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, data_base, static_datas, data_static_cb_0):\n",
    "        concatenated_static_data = pl.concat(static_datas, how='vertical')\n",
    "        merged_data = data_base.join(concatenated_static_data, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        static_datas = [self.load_data(path) for path in self.static_paths]\n",
    "        data_static_cb_0 = self.load_data(self.static_cb_0_path)\n",
    "        data_base = self.preprocess_base(data_base)\n",
    "        static_datas = [self.preprocess_static(data) for data in static_datas]\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(data_static_cb_0)\n",
    "        return self.merge_data(data_base, static_datas, data_static_cb_0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schema_path = \"C:/Users/afise/.git/CreditRiskModel/unified_schema.json\"\n",
    "    base_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_base.parquet\"\n",
    "    static_paths = [\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_2.parquet\"\n",
    "    ]\n",
    "    static_cb_0_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_cb_0.parquet\"\n",
    "    \n",
    "    pipeline = DataPipeline_Depth_0(base_path, static_paths, static_cb_0_path, schema_path)\n",
    "    Depth_0_test = pipeline.execute_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
