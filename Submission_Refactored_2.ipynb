{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = pl.lit(datetime(1970, 1, 1))\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(pl.lit(None))\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_paths, static_cb_0_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_paths = static_paths\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "\n",
    "    def load_data(self, path):\n",
    "        try:\n",
    "            return pl.read_parquet(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self):\n",
    "        data_frames = [self.load_data(path) for path in self.static_paths if path is not None]\n",
    "\n",
    "        # Find all unique columns across all data frames\n",
    "        unified_columns = set(col for df in data_frames for col in df.columns)\n",
    "\n",
    "        # Ensure each DataFrame has all the unified columns\n",
    "        for df in data_frames:\n",
    "            for col in unified_columns:\n",
    "                if col not in df.columns:\n",
    "                    # Assume string type for missing columns, adjust based on actual needs\n",
    "                    df[col] = pl.lit(\"\").alias(col) if 'D' not in col else pl.lit(None).cast(pl.Date)\n",
    "\n",
    "        # Concatenate ensuring all data frames have matching schema\n",
    "        data = pl.concat(data_frames, how='vertical')\n",
    "\n",
    "        # Convert dates and handle data types appropriately\n",
    "        date_columns = [col for col in data.columns if col.endswith('D')]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "            \n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "\n",
    "        # Select only the necessary columns, assuming date columns need to be kept\n",
    "        columns_to_keep = [col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns]\n",
    "        data = data.select(columns_to_keep)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        categorical_columns = ['education_1103M', 'maritalst_385M']\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data = data.with_columns(data[col].cast(pl.Categorical))\n",
    "        columns_to_drop = [col for col in data.columns if data[col].dtype == pl.Utf8 and col not in date_columns and col not in categorical_columns]\n",
    "        data = data.drop(columns_to_drop)\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, data_base, data_static, data_static_cb_0):\n",
    "        merged_data = data_base.join(data_static, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        data_static = self.preprocess_static()\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(self.load_data(self.static_cb_0_path))\n",
    "        Depth_0 = self.merge_data(data_base, data_static, data_static_cb_0)\n",
    "        return Depth_0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_base_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_base.parquet\"\n",
    "    train_static_paths = [\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_1.parquet\"\n",
    "    ]\n",
    "    train_static_cb_0_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_cb_0.parquet\"\n",
    "    \n",
    "    test_base_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_base.parquet\"\n",
    "    test_static_paths = [\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_2.parquet\"\n",
    "    ]\n",
    "    test_static_cb_0_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_cb_0.parquet\"\n",
    "\n",
    "    # Initialize pipelines for both training and test data\n",
    "    train_pipeline = DataPipeline_Depth_0(train_base_path, train_static_paths, train_static_cb_0_path)\n",
    "    test_pipeline = DataPipeline_Depth_0(test_base_path, test_static_paths, test_static_cb_0_path)\n",
    "    \n",
    "    # Execute pipelines\n",
    "    train_data = train_pipeline.execute_pipeline()\n",
    "    test_data = test_pipeline.execute_pipeline()\n",
    "    \n",
    "    # Output can be handled here, such as saving processed data or further analysis\n",
    "    print(\"Training Data Processed:\", train_data)\n",
    "    print(\"Test Data Processed:\", test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
