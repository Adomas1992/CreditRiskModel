{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = datetime(1970, 1, 1)\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(None)\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_0_0_path, static_0_1_path, static_cb_0_path, schema_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_0_0_path = static_0_0_path\n",
    "        self.static_0_1_path = static_0_1_path\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = {}\n",
    "\n",
    "    def load_data(self, path):\n",
    "        try:\n",
    "            df = pl.read_parquet(path)\n",
    "            self.update_schema(df)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_schema(self, dataframe):\n",
    "        for col, dtype in zip(dataframe.columns, dataframe.dtypes):\n",
    "            if col not in self.global_schema:\n",
    "                self.global_schema[col] = str(dtype)\n",
    "\n",
    "    def save_schema(self):\n",
    "        with open(self.schema_path, 'w') as file:\n",
    "            json.dump(self.global_schema, file)\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "        return data.select([col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns])\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        categorical_columns = ['education_1103M', 'maritalst_385M']\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data = data.with_columns(data[col].cast(pl.Categorical))\n",
    "        columns_to_drop = [col for col in data.columns if data[col].dtype == pl.Utf8 and col not in date_columns and col not in categorical_columns]\n",
    "        data = data.drop(columns_to_drop)\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, data_base, data_static_0_0, data_static_0_1, data_static_cb_0):\n",
    "        concatenated_data = pl.concat([data_static_0_0, data_static_0_1], how='vertical')\n",
    "        merged_data = data_base.join(concatenated_data, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        data_static_0_0 = self.load_data(self.static_0_0_path)\n",
    "        data_static_0_1 = self.load_data(self.static_0_1_path)\n",
    "        data_static_cb_0 = self.load_data(self.static_cb_0_path)\n",
    "        self.save_schema()\n",
    "        data_base = self.preprocess_base(data_base)\n",
    "        data_static_0_0 = self.preprocess_static(data_static_0_0)\n",
    "        data_static_0_1 = self.preprocess_static(data_static_0_1)\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(data_static_cb_0)\n",
    "        return self.merge_data(data_base, data_static_0_0, data_static_0_1, data_static_cb_0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schema_path = \"C:/Users/afise/.git/CreditRiskModel/unified_schema.json\"\n",
    "    pipeline = DataPipeline_Depth_0(\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_base.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/train/train_static_cb_0.parquet\",\n",
    "        schema_path\n",
    "    )\n",
    "    Depth_0 = pipeline.execute_pipeline()\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = datetime(1970, 1, 1)\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(None)\n",
    "\n",
    "def dtype_mapping(dtype_str):\n",
    "    mapping = {\n",
    "        'Int32': pl.Int32,\n",
    "        'Int64': pl.Int64,\n",
    "        'Float32': pl.Float32,\n",
    "        'Float64': pl.Float64,\n",
    "        'Utf8': pl.Utf8,\n",
    "        'Boolean': pl.Boolean,\n",
    "        'Date': pl.Date,\n",
    "        'Categorical': pl.Categorical\n",
    "    }\n",
    "    return mapping.get(dtype_str, pl.Utf8)\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_paths, static_cb_0_path, schema_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_paths = static_paths\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = self.load_schema()\n",
    "\n",
    "    def load_schema(self):\n",
    "        with open(self.schema_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        df = pl.read_parquet(path)\n",
    "        return self.ensure_schema(df)\n",
    "\n",
    "    def ensure_schema(self, dataframe):\n",
    "        for col, expected_dtype in self.global_schema.items():\n",
    "            expected_pl_dtype = dtype_mapping(expected_dtype)\n",
    "            if col in dataframe.columns:\n",
    "                if dataframe[col].dtype != expected_pl_dtype:\n",
    "                    dataframe = dataframe.with_columns(dataframe[col].cast(expected_pl_dtype))\n",
    "        return dataframe\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "        columns_to_keep = [col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns]\n",
    "        data = data.select(columns_to_keep)\n",
    "        return data\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        categorical_columns = ['education_1103M', 'maritalst_385M']\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data = data.with_columns(data[col].cast(pl.Categorical))\n",
    "        columns_to_drop = [col for col in data.columns if data[col].dtype == pl.Utf8 and col not in date_columns and col not in categorical_columns]\n",
    "        data = data.drop(columns_to_drop)\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, data_base, static_datas, data_static_cb_0):\n",
    "        concatenated_static_data = pl.concat(static_datas, how='vertical')\n",
    "        merged_data = data_base.join(concatenated_static_data, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        static_datas = [self.load_data(path) for path in self.static_paths]\n",
    "        data_static_cb_0 = self.load_data(self.static_cb_0_path)\n",
    "        data_base = self.preprocess_base(data_base)\n",
    "        static_datas = [self.preprocess_static(data) for data in static_datas]\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(data_static_cb_0)\n",
    "        return self.merge_data(data_base, static_datas, data_static_cb_0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schema_path = \"C:/Users/afise/.git/CreditRiskModel/unified_schema.json\"\n",
    "    base_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_base.parquet\"\n",
    "    static_paths = [\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_2.parquet\"\n",
    "    ]\n",
    "    static_cb_0_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_cb_0.parquet\"\n",
    "    \n",
    "    pipeline = DataPipeline_Depth_0(base_path, static_paths, static_cb_0_path, schema_path)\n",
    "    Depth_0_test = pipeline.execute_pipeline()\n",
    "    \n",
    "class DataPipeline_Depth_1:\n",
    "    def __init__(self, applprev_paths, other_path, deposit_path, person_path, debitcard_path, schema_path):\n",
    "        self.applprev_paths = applprev_paths\n",
    "        self.other_path = other_path\n",
    "        self.deposit_path = deposit_path\n",
    "        self.person_path = person_path\n",
    "        self.debitcard_path = debitcard_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def try_parse_date(col, fmt1, fmt2):\n",
    "        date1 = col.str.strptime(pl.Date, fmt1, strict=False)\n",
    "        date2 = col.str.strptime(pl.Date, fmt2, strict=False)\n",
    "        return pl.when(date1.is_not_null()).then(date1).otherwise(date2)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_ordinal(date):\n",
    "        return pl.when(date.is_not_null()).then(\n",
    "            (date.dt.year() * 365) + (date.dt.month() * 30) + date.dt.day()\n",
    "        ).otherwise(None)\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_applprev_1 = pl.concat([self.load_and_update_schema(path) for path in self.applprev_paths]).lazy()\n",
    "        self.train_other_1 = self.load_and_update_schema(self.other_path).lazy()\n",
    "        self.train_deposit_1 = self.load_and_update_schema(self.deposit_path).lazy()\n",
    "        self.train_person_1 = self.load_and_update_schema(self.person_path).lazy()\n",
    "        self.train_debitcard_1 = self.load_and_update_schema(self.debitcard_path).lazy()\n",
    "\n",
    "    def load_and_update_schema(self, path):\n",
    "        df = pl.read_parquet(path)\n",
    "        self.update_schema(df)\n",
    "        return df\n",
    "\n",
    "    def update_schema(self, dataframe):\n",
    "        for col, dtype in zip(dataframe.columns, dataframe.dtypes):\n",
    "            if col not in self.global_schema:\n",
    "                self.global_schema[col] = str(dtype)\n",
    "\n",
    "    def save_schema(self):\n",
    "        with open(self.schema_path, 'w') as file:\n",
    "            json.dump(self.global_schema, file)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        date_formats = (\"%m/%d/%Y\", \"%Y-%m-%d\")\n",
    "        date_columns = [\"approvaldate_319D\", \"dateactivated_425D\", \"creationdate_885D\", \"dtlastpmt_581D\", \"employedfrom_700D\", \"dtlastpmtallstes_3545839D\", \"firstnonzeroinstldate_307D\"]\n",
    "        self.train_applprev_1 = self.train_applprev_1.with_columns([\n",
    "            DataPipeline_Depth_1.try_parse_date(pl.col(col), *date_formats).alias(col) for col in date_columns\n",
    "        ]).group_by(\"case_id\").agg([\n",
    "            pl.col(\"actualdpd_943P\").mean().alias(\"actualdpd_943P_mean\"),\n",
    "            pl.col(\"annuity_853A\").sum().alias(\"annuity_853A_sum\"),\n",
    "            pl.col(\"childnum_21L\").sum().alias(\"childnum_21L_sum\"),\n",
    "            pl.col(\"credacc_actualbalance_314A\").mean().alias(\"credacc_actualbalance_314A_mean\"),\n",
    "            pl.col(\"credacc_credlmt_575A\").mean().alias(\"credacc_credlmt_575A_mean\"),\n",
    "            pl.col(\"credacc_maxhisbal_375A\").max().alias(\"credacc_maxhisbal_375A_max\"),\n",
    "            pl.col(\"credacc_minhisbal_90A\").min().alias(\"credacc_minhisbal_90A_min\"),\n",
    "            pl.col(\"credacc_transactions_402L\").sum().alias(\"credacc_transactions_402L_sum\"),\n",
    "            pl.col(\"credamount_590A\").mean().alias(\"credamount_590A_mean\"),\n",
    "            pl.col(\"currdebt_94A\").mean().alias(\"currdebt_94A_mean\"),\n",
    "            pl.col(\"downpmt_134A\").sum().alias(\"downpmt_134A_sum\"),\n",
    "            pl.col(\"mainoccupationinc_437A\").mean().alias(\"mainoccupationinc_437A_mean\"),\n",
    "            pl.col(\"outstandingdebt_522A\").sum().alias(\"outstandingdebt_522A_sum\"),\n",
    "            pl.col(\"pmtnum_8L\").max().alias(\"pmtnum_8L_max\"),\n",
    "            pl.col(\"tenor_203L\").min().alias(\"tenor_203L_min\"),\n",
    "            pl.col(\"isbidproduct_390L\").cast(pl.UInt32).sum().alias(\"isbidproduct_390L_sum\"),\n",
    "            pl.col(\"isdebitcard_527L\").cast(pl.UInt32).sum().alias(\"isdebitcard_527L_sum\"),\n",
    "            pl.col(\"credacc_status_367L\").n_unique().alias(\"credacc_status_367L_n_unique\"),\n",
    "            pl.col(\"credtype_587L\").n_unique().alias(\"credtype_587L_n_unique\"),\n",
    "            pl.col(\"education_1138M\").n_unique().alias(\"education_1138M_n_unique\"),\n",
    "            pl.col(\"familystate_726L\").n_unique().alias(\"familystate_726L_n_unique\"),\n",
    "            pl.col(\"postype_4733339M\").n_unique().alias(\"postype_4733339M_n_unique\"),\n",
    "            pl.col(\"profession_152M\").n_unique().alias(\"profession_152M_n_unique\"),\n",
    "            pl.col(\"rejectreason_755M\").n_unique().alias(\"rejectreason_755M_n_unique\"),\n",
    "            pl.col(\"rejectreasonclient_4145042M\").n_unique().alias(\"rejectreasonclient_4145042M_n_unique\"),\n",
    "            pl.col(\"status_219L\").n_unique().alias(\"status_219L_n_unique\"),\n",
    "            (pl.col(\"approvaldate_319D\").diff().abs().min()).alias(\"approval_to_activation_min_diff\"),\n",
    "            (pl.col(\"creationdate_885D\").diff().abs().min()).alias(\"creation_min_diff\"),\n",
    "            (pl.col(\"dtlastpmt_581D\").diff().abs().max()).alias(\"payment_max_diff\"),\n",
    "            pl.col(\"employedfrom_700D\").min().alias(\"earliest_employment_date\"),\n",
    "            pl.col(\"byoccupationinc_3656910L\").n_unique().alias(\"byoccupationinc_3656910L_n_unique\"),\n",
    "            pl.col(\"cancelreason_3545846M\").n_unique().alias(\"cancelreason_3545846M_n_unique\"),\n",
    "            pl.col(\"district_544M\").n_unique().alias(\"district_544M_n_unique\"),\n",
    "            pl.col(\"dtlastpmtallstes_3545839D\").min().alias(\"earliest_last_payment_date\"),\n",
    "            pl.col(\"firstnonzeroinstldate_307D\").min().alias(\"earliest_first_nonzero_installment_date\"),\n",
    "            pl.col(\"inittransactioncode_279L\").n_unique().alias(\"inittransactioncode_279L_n_unique\"),\n",
    "            pl.col(\"maxdpdtolerance_577P\").max().alias(\"maximum_dpd_tolerance\"),\n",
    "            pl.col(\"revolvingaccount_394A\").sum().alias(\"sum_revolving_accounts\")\n",
    "        ])\n",
    "\n",
    "        self.train_other_1 = self.train_other_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"amtdebitincoming_4809443A\").sum().alias(\"sum_amtdebitincoming\"),\n",
    "            pl.col(\"amtdebitoutgoing_4809440A\").sum().alias(\"sum_amtdebitoutgoing\"),\n",
    "            pl.col(\"amtdepositbalance_4809441A\").mean().alias(\"avg_amtdepositbalance\"),\n",
    "            pl.col(\"amtdepositincoming_4809444A\").sum().alias(\"sum_amtdepositincoming\"),\n",
    "            pl.col(\"amtdepositoutgoing_4809442A\").sum().alias(\"sum_amtdepositoutgoing\")\n",
    "        ])\n",
    "\n",
    "        self.train_deposit_1 = self.train_deposit_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"amount_416A\").mean().alias(\"average_amount\"),\n",
    "            pl.count(\"openingdate_313D\").alias(\"open_contracts_count\"),\n",
    "            pl.count(\"contractenddate_991D\").alias(\"closed_contracts_count\")\n",
    "        ])\n",
    "\n",
    "        date_format = (\"%m/%d/%Y\", \"%Y-%m-%d\")\n",
    "        self.train_person_1 = self.train_person_1.with_columns(\n",
    "            DataPipeline_Depth_1.try_parse_date(pl.col(\"empl_employedfrom_271D\"), *date_format).alias(\"empl_employedfrom_271D\")\n",
    "        )\n",
    "        self.train_person_1 = self.train_person_1.with_columns(\n",
    "            DataPipeline_Depth_1.convert_to_ordinal(pl.col(\"empl_employedfrom_271D\")).alias(\"ordinal_employedfrom_271D\")\n",
    "        )\n",
    "        self.train_person_1 = self.train_person_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"birth_259D\").n_unique().alias(\"unique_birth_dates\"),\n",
    "            pl.col(\"birthdate_87D\").n_unique().alias(\"unique_birth_dates_87D\"),\n",
    "            pl.col(\"childnum_185L\").max().alias(\"max_children\"),\n",
    "            pl.col(\"education_927M\").n_unique().alias(\"unique_educations\"),\n",
    "            pl.col(\"empl_employedtotal_800L\").n_unique().alias(\"avg_employment_length\"),\n",
    "            pl.col(\"mainoccupationinc_384A\").sum().alias(\"total_main_income\"),\n",
    "            pl.col(\"gender_992L\").n_unique().alias(\"unique_genders\"),\n",
    "            pl.col(\"housetype_905L\").n_unique().alias(\"unique_house_types\"),\n",
    "            pl.col(\"housingtype_772L\").n_unique().alias(\"unique_housing_types\"),\n",
    "            pl.col(\"incometype_1044T\").n_unique().alias(\"unique_income_types\"),\n",
    "            pl.col(\"maritalst_703L\").n_unique().alias(\"unique_marital_statuses\"),\n",
    "            pl.col(\"persontype_1072L\").n_unique().alias(\"unique_person_types_1072L\"),\n",
    "            pl.col(\"persontype_792L\").n_unique().alias(\"unique_person_types_792L\"),\n",
    "            pl.col(\"relationshiptoclient_415T\").n_unique().alias(\"unique_relationships_415T\"),\n",
    "            pl.col(\"relationshiptoclient_642T\").n_unique().alias(\"unique_relationships_642T\"),\n",
    "            pl.col(\"remitter_829L\").sum().alias(\"sum_remitters\"),\n",
    "            pl.col(\"role_1084L\").n_unique().alias(\"unique_roles_1084L\"),\n",
    "            pl.col(\"role_993L\").n_unique().alias(\"unique_roles_993L\"),\n",
    "            pl.col(\"safeguarantyflag_411L\").sum().alias(\"sum_safeguaranty_flags\"),\n",
    "            pl.col(\"sex_738L\").n_unique().alias(\"unique_sexes\"),\n",
    "            pl.col(\"type_25L\").n_unique().alias(\"unique_contact_types\"),\n",
    "            pl.col(\"contaddr_district_15M\").n_unique().alias(\"unique_contact_address_districts\"),\n",
    "            pl.col(\"empladdr_district_926M\").n_unique().alias(\"unique_employer_address_districts\"),\n",
    "            pl.col(\"registaddr_district_1083M\").n_unique().alias(\"unique_registered_address_districts\"),\n",
    "            pl.col(\"isreference_387L\").sum().alias(\"sum_is_reference_flags\"),\n",
    "            pl.col(\"empl_industry_691L\").n_unique().alias(\"unique_industries\"),\n",
    "            pl.col(\"empladdr_zipcode_114M\").n_unique().alias(\"unique_employer_zipcodes\"),\n",
    "            pl.col(\"contaddr_zipcode_807M\").n_unique().alias(\"unique_contact_zipcodes\"),\n",
    "            pl.col(\"registaddr_zipcode_184M\").n_unique().alias(\"unique_registered_zipcodes\"),\n",
    "            pl.col(\"language1_981M\").n_unique().alias(\"unique_languages\"),\n",
    "            pl.col(\"familystate_447L\").n_unique().alias(\"unique_family_states\"),\n",
    "            pl.col(\"contaddr_matchlist_1032L\").sum().alias(\"sum_contact_address_matchlist\"),\n",
    "            pl.col(\"contaddr_smempladdr_334L\").sum().alias(\"sum_contact_same_employer_address\"),\n",
    "            pl.col(\"personindex_1023L\").n_unique().alias(\"unique_person_indices\"),\n",
    "            pl.col(\"ordinal_employedfrom_271D\").max().alias(\"latest_employment_date_ordinal\")\n",
    "        ])\n",
    "\n",
    "        self.train_debitcard_1 = self.train_debitcard_1.with_columns([\n",
    "            DataPipeline_Depth_1.convert_to_ordinal(\n",
    "                pl.col(\"openingdate_857D\").str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "            ).alias(\"ordinal_openingdate\")\n",
    "        ])\n",
    "\n",
    "        self.train_debitcard_1 = self.train_debitcard_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"last180dayaveragebalance_704A\").sum().alias(\"total_180dayaveragebalance\"),\n",
    "            pl.col(\"last180dayturnover_1134A\").sum().alias(\"total_180dayturnover\"),\n",
    "            pl.col(\"last30dayturnover_651A\").sum().alias(\"total_30dayturnover\"),\n",
    "            pl.min(\"ordinal_openingdate\").alias(\"earliest_openingdate\")\n",
    "        ])\n",
    "\n",
    "    def merge_data(self):\n",
    "        df_joined = self.train_applprev_1.join(self.train_other_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_deposit_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_person_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_debitcard_1, on=\"case_id\", how=\"left\")\n",
    "        \n",
    "        duration_columns = [\"approval_to_activation_min_diff\", \"creation_min_diff\", \"payment_max_diff\"]\n",
    "        for column in duration_columns:\n",
    "            df_joined = df_joined.with_columns(\n",
    "                pl.col(column).str.replace(\"d\", \"\").cast(pl.Int64) * 1440\n",
    "            )\n",
    "\n",
    "        column_names = df_joined.columns\n",
    "        column_types = df_joined.dtypes\n",
    "        date_columns = [name for name, dtype in zip(column_names, column_types) if dtype == pl.Date]\n",
    "        for col in date_columns:\n",
    "            df_joined = df_joined.with_columns(\n",
    "                DataPipeline_Depth_1.convert_to_ordinal(pl.col(col)).alias(col)\n",
    "            )\n",
    "\n",
    "        return df_joined.collect()\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        merged_data = self.merge_data()\n",
    "        self.save_schema()\n",
    "        return merged_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    applprev_paths = [\n",
    "        \"Data/parquet_files/train/train_applprev_1_0.parquet\",\n",
    "        \"Data/parquet_files/train/train_applprev_1_1.parquet\"\n",
    "    ]\n",
    "    other_path = \"Data/parquet_files/train/train_other_1.parquet\"\n",
    "    deposit_path = \"Data/parquet_files/train/train_deposit_1.parquet\"\n",
    "    person_path = \"Data/parquet_files/train/train_person_1.parquet\"\n",
    "    debitcard_path = \"Data/parquet_files/train/train_debitcard_1.parquet\"\n",
    "    schema_path = \"unified_schema_2.json\"\n",
    "\n",
    "    pipeline = DataPipeline_Depth_1(applprev_paths, other_path, deposit_path, person_path, debitcard_path, schema_path)\n",
    "    Depth_1 = pipeline.execute_pipeline()\n",
    "    \n",
    "class DataPipeline_Depth_1:\n",
    "    def __init__(self, applprev_paths, other_path, deposit_path, person_path, debitcard_path, schema_path):\n",
    "        self.applprev_paths = applprev_paths\n",
    "        self.other_path = other_path\n",
    "        self.deposit_path = deposit_path\n",
    "        self.person_path = person_path\n",
    "        self.debitcard_path = debitcard_path\n",
    "        self.schema_path = schema_path\n",
    "        self.global_schema = self.load_schema()\n",
    "\n",
    "    def load_schema(self):\n",
    "        with open(self.schema_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dtype_mapping(dtype_str):\n",
    "        mapping = {\n",
    "            'Int32': pl.Int32,\n",
    "            'Int64': pl.Int64,\n",
    "            'Float32': pl.Float32,\n",
    "            'Float64': pl.Float64,\n",
    "            'Utf8': pl.Utf8,\n",
    "            'Boolean': pl.Boolean,\n",
    "            'Date': pl.Date,\n",
    "            'Categorical': pl.Categorical\n",
    "        }\n",
    "        return mapping.get(dtype_str, pl.Utf8)\n",
    "\n",
    "    @staticmethod\n",
    "    def try_parse_date(col, fmt1, fmt2):\n",
    "        date1 = col.str.strptime(pl.Date, fmt1, strict=False)\n",
    "        date2 = col.str.strptime(pl.Date, fmt2, strict=False)\n",
    "        return pl.when(date1.is_not_null()).then(date1).otherwise(date2)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_ordinal(date):\n",
    "        return pl.when(date.is_not_null()).then(\n",
    "            (date.dt.year() * 365) + (date.dt.month() * 30) + date.dt.day()\n",
    "        ).otherwise(None)\n",
    "    \n",
    "    def load_and_ensure_schema(self, path):\n",
    "        df = pl.read_parquet(path)\n",
    "        for col, expected_dtype in self.global_schema.items():\n",
    "            expected_pl_dtype = self.dtype_mapping(expected_dtype)\n",
    "            if col in df.columns:\n",
    "                if df[col].dtype != expected_pl_dtype:\n",
    "                    df = df.with_columns(df[col].cast(expected_pl_dtype))\n",
    "            else:\n",
    "                df = df.with_columns(pl.lit(None).cast(expected_pl_dtype))\n",
    "        return df\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_applprev_1 = pl.concat([self.load_and_ensure_schema(path) for path in self.applprev_paths]).lazy()\n",
    "        self.train_other_1 = self.load_and_ensure_schema(self.other_path).lazy()\n",
    "        self.train_deposit_1 = self.load_and_ensure_schema(self.deposit_path).lazy()\n",
    "        self.train_person_1 = self.load_and_ensure_schema(self.person_path).lazy()\n",
    "        self.train_debitcard_1 = self.load_and_ensure_schema(self.debitcard_path).lazy()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        date_formats = (\"%m/%d/%Y\", \"%Y-%m-%d\")\n",
    "        date_columns = [\"approvaldate_319D\", \"dateactivated_425D\", \"creationdate_885D\", \"dtlastpmt_581D\", \"employedfrom_700D\", \"dtlastpmtallstes_3545839D\", \"firstnonzeroinstldate_307D\"]\n",
    "        self.train_applprev_1 = self.train_applprev_1.with_columns([\n",
    "            DataPipeline_Depth_1.try_parse_date(pl.col(col), *date_formats).alias(col) for col in date_columns\n",
    "        ]).group_by(\"case_id\").agg([\n",
    "            pl.col(\"actualdpd_943P\").mean().alias(\"actualdpd_943P_mean\"),\n",
    "            pl.col(\"annuity_853A\").sum().alias(\"annuity_853A_sum\"),\n",
    "            pl.col(\"childnum_21L\").sum().alias(\"childnum_21L_sum\"),\n",
    "            pl.col(\"credacc_actualbalance_314A\").mean().alias(\"credacc_actualbalance_314A_mean\"),\n",
    "            pl.col(\"credacc_credlmt_575A\").mean().alias(\"credacc_credlmt_575A_mean\"),\n",
    "            pl.col(\"credacc_maxhisbal_375A\").max().alias(\"credacc_maxhisbal_375A_max\"),\n",
    "            pl.col(\"credacc_minhisbal_90A\").min().alias(\"credacc_minhisbal_90A_min\"),\n",
    "            pl.col(\"credacc_transactions_402L\").sum().alias(\"credacc_transactions_402L_sum\"),\n",
    "            pl.col(\"credamount_590A\").mean().alias(\"credamount_590A_mean\"),\n",
    "            pl.col(\"currdebt_94A\").mean().alias(\"currdebt_94A_mean\"),\n",
    "            pl.col(\"downpmt_134A\").sum().alias(\"downpmt_134A_sum\"),\n",
    "            pl.col(\"mainoccupationinc_437A\").mean().alias(\"mainoccupationinc_437A_mean\"),\n",
    "            pl.col(\"outstandingdebt_522A\").sum().alias(\"outstandingdebt_522A_sum\"),\n",
    "            pl.col(\"pmtnum_8L\").max().alias(\"pmtnum_8L_max\"),\n",
    "            pl.col(\"tenor_203L\").min().alias(\"tenor_203L_min\"),\n",
    "            pl.col(\"isbidproduct_390L\").cast(pl.UInt32).sum().alias(\"isbidproduct_390L_sum\"),\n",
    "            pl.col(\"isdebitcard_527L\").cast(pl.UInt32).sum().alias(\"isdebitcard_527L_sum\"),\n",
    "            pl.col(\"credacc_status_367L\").n_unique().alias(\"credacc_status_367L_n_unique\"),\n",
    "            pl.col(\"credtype_587L\").n_unique().alias(\"credtype_587L_n_unique\"),\n",
    "            pl.col(\"education_1138M\").n_unique().alias(\"education_1138M_n_unique\"),\n",
    "            pl.col(\"familystate_726L\").n_unique().alias(\"familystate_726L_n_unique\"),\n",
    "            pl.col(\"postype_4733339M\").n_unique().alias(\"postype_4733339M_n_unique\"),\n",
    "            pl.col(\"profession_152M\").n_unique().alias(\"profession_152M_n_unique\"),\n",
    "            pl.col(\"rejectreason_755M\").n_unique().alias(\"rejectreason_755M_n_unique\"),\n",
    "            pl.col(\"rejectreasonclient_4145042M\").n_unique().alias(\"rejectreasonclient_4145042M_n_unique\"),\n",
    "            pl.col(\"status_219L\").n_unique().alias(\"status_219L_n_unique\"),\n",
    "            (pl.col(\"approvaldate_319D\").diff().abs().min()).alias(\"approval_to_activation_min_diff\"),\n",
    "            (pl.col(\"creationdate_885D\").diff().abs().min()).alias(\"creation_min_diff\"),\n",
    "            (pl.col(\"dtlastpmt_581D\").diff().abs().max()).alias(\"payment_max_diff\"),\n",
    "            pl.col(\"employedfrom_700D\").min().alias(\"earliest_employment_date\"),\n",
    "            pl.col(\"byoccupationinc_3656910L\").n_unique().alias(\"byoccupationinc_3656910L_n_unique\"),\n",
    "            pl.col(\"cancelreason_3545846M\").n_unique().alias(\"cancelreason_3545846M_n_unique\"),\n",
    "            pl.col(\"district_544M\").n_unique().alias(\"district_544M_n_unique\"),\n",
    "            pl.col(\"dtlastpmtallstes_3545839D\").min().alias(\"earliest_last_payment_date\"),\n",
    "            pl.col(\"firstnonzeroinstldate_307D\").min().alias(\"earliest_first_nonzero_installment_date\"),\n",
    "            pl.col(\"inittransactioncode_279L\").n_unique().alias(\"inittransactioncode_279L_n_unique\"),\n",
    "            pl.col(\"maxdpdtolerance_577P\").max().alias(\"maximum_dpd_tolerance\"),\n",
    "            pl.col(\"revolvingaccount_394A\").sum().alias(\"sum_revolving_accounts\")\n",
    "        ])\n",
    "\n",
    "        self.train_other_1 = self.train_other_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"amtdebitincoming_4809443A\").sum().alias(\"sum_amtdebitincoming\"),\n",
    "            pl.col(\"amtdebitoutgoing_4809440A\").sum().alias(\"sum_amtdebitoutgoing\"),\n",
    "            pl.col(\"amtdepositbalance_4809441A\").mean().alias(\"avg_amtdepositbalance\"),\n",
    "            pl.col(\"amtdepositincoming_4809444A\").sum().alias(\"sum_amtdepositincoming\"),\n",
    "            pl.col(\"amtdepositoutgoing_4809442A\").sum().alias(\"sum_amtdepositoutgoing\")\n",
    "        ])\n",
    "\n",
    "        self.train_deposit_1 = self.train_deposit_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"amount_416A\").mean().alias(\"average_amount\"),\n",
    "            pl.count(\"openingdate_313D\").alias(\"open_contracts_count\"),\n",
    "            pl.count(\"contractenddate_991D\").alias(\"closed_contracts_count\")\n",
    "        ])\n",
    "\n",
    "        date_format = (\"%m/%d/%Y\", \"%Y-%m-%d\")\n",
    "        self.train_person_1 = self.train_person_1.with_columns(\n",
    "            DataPipeline_Depth_1.try_parse_date(pl.col(\"empl_employedfrom_271D\"), *date_format).alias(\"empl_employedfrom_271D\")\n",
    "        )\n",
    "        self.train_person_1 = self.train_person_1.with_columns(\n",
    "            DataPipeline_Depth_1.convert_to_ordinal(pl.col(\"empl_employedfrom_271D\")).alias(\"ordinal_employedfrom_271D\")\n",
    "        )\n",
    "        self.train_person_1 = self.train_person_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"birth_259D\").n_unique().alias(\"unique_birth_dates\"),\n",
    "            pl.col(\"birthdate_87D\").n_unique().alias(\"unique_birth_dates_87D\"),\n",
    "            pl.col(\"childnum_185L\").max().alias(\"max_children\"),\n",
    "            pl.col(\"education_927M\").n_unique().alias(\"unique_educations\"),\n",
    "            pl.col(\"empl_employedtotal_800L\").n_unique().alias(\"avg_employment_length\"),\n",
    "            pl.col(\"mainoccupationinc_384A\").sum().alias(\"total_main_income\"),\n",
    "            pl.col(\"gender_992L\").n_unique().alias(\"unique_genders\"),\n",
    "            pl.col(\"housetype_905L\").n_unique().alias(\"unique_house_types\"),\n",
    "            pl.col(\"housingtype_772L\").n_unique().alias(\"unique_housing_types\"),\n",
    "            pl.col(\"incometype_1044T\").n_unique().alias(\"unique_income_types\"),\n",
    "            pl.col(\"maritalst_703L\").n_unique().alias(\"unique_marital_statuses\"),\n",
    "            pl.col(\"persontype_1072L\").n_unique().alias(\"unique_person_types_1072L\"),\n",
    "            pl.col(\"persontype_792L\").n_unique().alias(\"unique_person_types_792L\"),\n",
    "            pl.col(\"relationshiptoclient_415T\").n_unique().alias(\"unique_relationships_415T\"),\n",
    "            pl.col(\"relationshiptoclient_642T\").n_unique().alias(\"unique_relationships_642T\"),\n",
    "            pl.col(\"remitter_829L\").sum().alias(\"sum_remitters\"),\n",
    "            pl.col(\"role_1084L\").n_unique().alias(\"unique_roles_1084L\"),\n",
    "            pl.col(\"role_993L\").n_unique().alias(\"unique_roles_993L\"),\n",
    "            pl.col(\"safeguarantyflag_411L\").sum().alias(\"sum_safeguaranty_flags\"),\n",
    "            pl.col(\"sex_738L\").n_unique().alias(\"unique_sexes\"),\n",
    "            pl.col(\"type_25L\").n_unique().alias(\"unique_contact_types\"),\n",
    "            pl.col(\"contaddr_district_15M\").n_unique().alias(\"unique_contact_address_districts\"),\n",
    "            pl.col(\"empladdr_district_926M\").n_unique().alias(\"unique_employer_address_districts\"),\n",
    "            pl.col(\"registaddr_district_1083M\").n_unique().alias(\"unique_registered_address_districts\"),\n",
    "            pl.col(\"isreference_387L\").sum().alias(\"sum_is_reference_flags\"),\n",
    "            pl.col(\"empl_industry_691L\").n_unique().alias(\"unique_industries\"),\n",
    "            pl.col(\"empladdr_zipcode_114M\").n_unique().alias(\"unique_employer_zipcodes\"),\n",
    "            pl.col(\"contaddr_zipcode_807M\").n_unique().alias(\"unique_contact_zipcodes\"),\n",
    "            pl.col(\"registaddr_zipcode_184M\").n_unique().alias(\"unique_registered_zipcodes\"),\n",
    "            pl.col(\"language1_981M\").n_unique().alias(\"unique_languages\"),\n",
    "            pl.col(\"familystate_447L\").n_unique().alias(\"unique_family_states\"),\n",
    "            pl.col(\"contaddr_matchlist_1032L\").sum().alias(\"sum_contact_address_matchlist\"),\n",
    "            pl.col(\"contaddr_smempladdr_334L\").sum().alias(\"sum_contact_same_employer_address\"),\n",
    "            pl.col(\"personindex_1023L\").n_unique().alias(\"unique_person_indices\"),\n",
    "            pl.col(\"ordinal_employedfrom_271D\").max().alias(\"latest_employment_date_ordinal\")\n",
    "        ])\n",
    "\n",
    "        self.train_debitcard_1 = self.train_debitcard_1.with_columns([\n",
    "            DataPipeline_Depth_1.convert_to_ordinal(\n",
    "                pl.col(\"openingdate_857D\").str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "            ).alias(\"ordinal_openingdate\")\n",
    "        ])\n",
    "\n",
    "        self.train_debitcard_1 = self.train_debitcard_1.group_by(\"case_id\").agg([\n",
    "            pl.col(\"last180dayaveragebalance_704A\").sum().alias(\"total_180dayaveragebalance\"),\n",
    "            pl.col(\"last180dayturnover_1134A\").sum().alias(\"total_180dayturnover\"),\n",
    "            pl.col(\"last30dayturnover_651A\").sum().alias(\"total_30dayturnover\"),\n",
    "            pl.min(\"ordinal_openingdate\").alias(\"earliest_openingdate\")\n",
    "        ])\n",
    "\n",
    "    def merge_data(self):\n",
    "        df_joined = self.train_applprev_1.join(self.train_other_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_deposit_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_person_1, on=\"case_id\", how=\"left\")\n",
    "        df_joined = df_joined.join(self.train_debitcard_1, on=\"case_id\", how=\"left\")\n",
    "        \n",
    "        duration_columns = [\"approval_to_activation_min_diff\", \"creation_min_diff\", \"payment_max_diff\"]\n",
    "        for column in duration_columns:\n",
    "            df_joined = df_joined.with_columns(\n",
    "                pl.col(column).str.replace(\"d\", \"\").cast(pl.Int64) * 1440\n",
    "            )\n",
    "\n",
    "        column_names = df_joined.columns\n",
    "        column_types = df_joined.dtypes\n",
    "        date_columns = [name for name, dtype in zip(column_names, column_types) if dtype == pl.Date]\n",
    "        for col in date_columns:\n",
    "            df_joined = df_joined.with_columns(\n",
    "                DataPipeline_Depth_1.convert_to_ordinal(pl.col(col)).alias(col)\n",
    "            )\n",
    "\n",
    "        return df_joined.collect()\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        return self.merge_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    applprev_paths = [\n",
    "        \"Data/parquet_files/test/test_applprev_1_0.parquet\",\n",
    "        \"Data/parquet_files/test/test_applprev_1_1.parquet\",\n",
    "        \"Data/parquet_files/test/test_applprev_1_2.parquet\"\n",
    "    ]\n",
    "    other_path = \"Data/parquet_files/test/test_other_1.parquet\"\n",
    "    deposit_path = \"Data/parquet_files/test/test_deposit_1.parquet\"\n",
    "    person_path = \"Data/parquet_files/test/test_person_1.parquet\"\n",
    "    debitcard_path = \"Data/parquet_files/test/test_debitcard_1.parquet\"\n",
    "    schema_path = \"unified_schema_2.json\"\n",
    "\n",
    "    pipeline = DataPipeline_Depth_1(applprev_paths, other_path, deposit_path, person_path, debitcard_path, schema_path)\n",
    "    Depth_1_test = pipeline.execute_pipeline()\n",
    "    \n",
    "class DataPipeline_Depth_2:\n",
    "    def __init__(self, applprev_path, person_path):\n",
    "        self.applprev_path = applprev_path\n",
    "        self.person_path = person_path\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            train_applprev_2 = pl.read_parquet(self.applprev_path)\n",
    "            train_person_2 = pl.read_parquet(self.person_path)\n",
    "            return train_applprev_2, train_person_2\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_applprev(self, data):\n",
    "        data = data.group_by([\"case_id\", \"num_group1\"]).agg([\n",
    "            pl.col(\"conts_type_509L\").unique().count().alias(\"unique_contact_types\"),\n",
    "            pl.col(\"cacccardblochreas_147M\").max().alias(\"first_cacccardblochreas_147M\"),\n",
    "            pl.col(\"credacc_cards_status_52L\").max().alias(\"first_credacc_cards_status_52L\")\n",
    "        ]).with_columns(\n",
    "            pl.col('first_credacc_cards_status_52L')\n",
    "                .fill_null('UNKNOWN')\n",
    "                .alias('status')\n",
    "        ).with_columns([\n",
    "            (pl.col('status') == 'ACTIVE').cast(pl.Int32).alias('is_active'),\n",
    "            (pl.col('status') == 'CANCELLED').cast(pl.Int32).alias('is_cancelled')\n",
    "        ]).group_by('case_id').agg([\n",
    "            pl.col('unique_contact_types').max().alias('max_unique_contact_type'),\n",
    "            pl.col('first_cacccardblochreas_147M').n_unique().alias('n_unique_cacccardblochreas_147M'),\n",
    "            pl.sum('is_cancelled').alias('total_cancelled'),\n",
    "            pl.sum('is_active').alias('total_active')\n",
    "        ])\n",
    "        return data\n",
    "\n",
    "    def preprocess_person(self, data):\n",
    "        data = data.group_by('case_id').agg([\n",
    "            pl.col('addres_district_368M').n_unique().alias('n_unique_addres_district_368M'),\n",
    "            pl.col('addres_role_871L').n_unique().alias('n_unique_addres_role_871L'),\n",
    "            pl.col('addres_zip_823M').n_unique().alias('n_unique_addres_zip_823M'),\n",
    "            pl.col('conts_role_79M').n_unique().alias('n_unique_conts_role_79M'),\n",
    "            pl.col('empls_economicalst_849M').n_unique().alias('n_unique_empls_economicalst_849M'),\n",
    "            pl.col('empls_employedfrom_796D').n_unique().alias('n_unique_empls_employedfrom_796D'),\n",
    "            pl.col('empls_employer_name_740M').n_unique().alias('n_unique_empls_employer_name_740M'),\n",
    "            pl.col('relatedpersons_role_762T').n_unique().alias('n_unique_relatedpersons_role_762T')\n",
    "        ])\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, applprev_data, person_data):\n",
    "        merged_data = applprev_data.join(\n",
    "            person_data,\n",
    "            on='case_id',\n",
    "            how='left'\n",
    "        )\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        applprev_data, person_data = self.load_data()\n",
    "        applprev_data = self.preprocess_applprev(applprev_data)\n",
    "        person_data = self.preprocess_person(person_data)\n",
    "        Depth_2 = self.merge_data(applprev_data, person_data)\n",
    "        return Depth_2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    applprev_path = \"Data/parquet_files/train/train_applprev_2.parquet\"\n",
    "    person_path = \"Data/parquet_files/train/train_person_2.parquet\"\n",
    "    pipeline = DataPipeline_Depth_2(applprev_path, person_path)\n",
    "    Depth_2 = pipeline.execute_pipeline()\n",
    "    \n",
    "class DataPipeline_Depth_2:\n",
    "    def __init__(self, applprev_path, person_path):\n",
    "        self.applprev_path = applprev_path\n",
    "        self.person_path = person_path\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            train_applprev_2 = pl.read_parquet(self.applprev_path)\n",
    "            train_person_2 = pl.read_parquet(self.person_path)\n",
    "            return train_applprev_2, train_person_2\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_applprev(self, data):\n",
    "        data = data.group_by([\"case_id\", \"num_group1\"]).agg([\n",
    "            pl.col(\"conts_type_509L\").unique().count().alias(\"unique_contact_types\"),\n",
    "            pl.col(\"cacccardblochreas_147M\").max().alias(\"first_cacccardblochreas_147M\"),\n",
    "            pl.col(\"credacc_cards_status_52L\").max().alias(\"first_credacc_cards_status_52L\")\n",
    "        ]).with_columns(\n",
    "            pl.col('first_credacc_cards_status_52L')\n",
    "                .fill_null('UNKNOWN')\n",
    "                .alias('status')\n",
    "        ).with_columns([\n",
    "            (pl.col('status') == 'ACTIVE').cast(pl.Int32).alias('is_active'),\n",
    "            (pl.col('status') == 'CANCELLED').cast(pl.Int32).alias('is_cancelled')\n",
    "        ]).group_by('case_id').agg([\n",
    "            pl.col('unique_contact_types').max().alias('max_unique_contact_type'),\n",
    "            pl.col('first_cacccardblochreas_147M').n_unique().alias('n_unique_cacccardblochreas_147M'),\n",
    "            pl.sum('is_cancelled').alias('total_cancelled'),\n",
    "            pl.sum('is_active').alias('total_active')\n",
    "        ])\n",
    "        return data\n",
    "\n",
    "    def preprocess_person(self, data):\n",
    "        data = data.group_by('case_id').agg([\n",
    "            pl.col('addres_district_368M').n_unique().alias('n_unique_addres_district_368M'),\n",
    "            pl.col('addres_role_871L').n_unique().alias('n_unique_addres_role_871L'),\n",
    "            pl.col('addres_zip_823M').n_unique().alias('n_unique_addres_zip_823M'),\n",
    "            pl.col('conts_role_79M').n_unique().alias('n_unique_conts_role_79M'),\n",
    "            pl.col('empls_economicalst_849M').n_unique().alias('n_unique_empls_economicalst_849M'),\n",
    "            pl.col('empls_employedfrom_796D').n_unique().alias('n_unique_empls_employedfrom_796D'),\n",
    "            pl.col('empls_employer_name_740M').n_unique().alias('n_unique_empls_employer_name_740M'),\n",
    "            pl.col('relatedpersons_role_762T').n_unique().alias('n_unique_relatedpersons_role_762T')\n",
    "        ])\n",
    "        return data\n",
    "\n",
    "    def merge_data(self, applprev_data, person_data):\n",
    "        merged_data = applprev_data.join(\n",
    "            person_data,\n",
    "            on='case_id',\n",
    "            how='left'\n",
    "        )\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        applprev_data, person_data = self.load_data()\n",
    "        applprev_data = self.preprocess_applprev(applprev_data)\n",
    "        person_data = self.preprocess_person(person_data)\n",
    "        Depth_2 = self.merge_data(applprev_data, person_data)\n",
    "        return Depth_2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    applprev_path = \"Data/parquet_files/test/test_applprev_2.parquet\"\n",
    "    person_path = \"Data/parquet_files/test/test_person_2.parquet\"\n",
    "    pipeline = DataPipeline_Depth_2(applprev_path, person_path)\n",
    "    Depth_2_test = pipeline.execute_pipeline()\n",
    "\n",
    "Internal_Final = Depth_0.join(Depth_1, on='case_id', how='left')\n",
    "Internal_Final = Internal_Final.join(Depth_2, on='case_id', how='left')\n",
    "\n",
    "Internal_Final_test = Depth_0_test.join(Depth_1_test, on='case_id', how='left')\n",
    "Internal_Final_test = Internal_Final_test.join(Depth_2_test, on='case_id', how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
