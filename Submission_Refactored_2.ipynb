{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_to_ordinal(date_column, date_format):\n",
    "    epoch_start = pl.lit(datetime(1970, 1, 1))\n",
    "    date_parsed = date_column.str.strptime(pl.Date, date_format, strict=False)\n",
    "    return pl.when(date_parsed.is_not_null()).then(\n",
    "        (date_parsed - epoch_start).dt.total_days()\n",
    "    ).otherwise(pl.lit(None))\n",
    "\n",
    "class DataPipeline_Depth_0:\n",
    "    def __init__(self, base_path, static_paths, static_cb_0_path):\n",
    "        self.base_path = base_path\n",
    "        self.static_paths = static_paths\n",
    "        self.static_cb_0_path = static_cb_0_path\n",
    "\n",
    "    def load_data(self, path):\n",
    "        try:\n",
    "            if isinstance(path, list):\n",
    "                # Load and concatenate multiple datasets handling schema alignment\n",
    "                return pl.concat([self.adjust_schema(pl.read_parquet(p)) for p in path], how='vertical')\n",
    "            return self.adjust_schema(pl.read_parquet(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def adjust_schema(self, df):\n",
    "        # Define the expected schema from training data or predefined\n",
    "        expected_schema = {\n",
    "            # Assuming schema defined elsewhere or load from training metadata\n",
    "        }\n",
    "        # Adjust the schema dynamically based on expected schema\n",
    "        for col, dtype in expected_schema.items():\n",
    "            if col not in df.columns:\n",
    "                df = df.with_column(pl.lit(None, dtype=dtype).alias(col))\n",
    "            else:\n",
    "                if df[col].dtype != dtype:\n",
    "                    df = df.with_column(df[col].cast(dtype))\n",
    "        return df\n",
    "\n",
    "    def preprocess_base(self, data):\n",
    "        data = data.with_columns(\n",
    "            convert_to_ordinal(pl.col('date_decision'), '%Y-%m-%d').alias('date_decision_ordinal')\n",
    "        )\n",
    "        data = data.drop(['date_decision'])\n",
    "        return data\n",
    "\n",
    "    def preprocess_static(self, data):\n",
    "        date_columns = [col for col in data.columns if col.endswith('D') and data[col].dtype == pl.Utf8]\n",
    "        for col in date_columns:\n",
    "            data = data.with_columns(\n",
    "                convert_to_ordinal(pl.col(col), '%Y-%m-%d').alias(col)\n",
    "            )\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == pl.Boolean:\n",
    "                data = data.with_columns(data[col].cast(pl.Int32).alias(col))\n",
    "        columns_to_keep = [col for col in data.columns if data[col].dtype != pl.Utf8 or col in date_columns]\n",
    "        data = data.select(columns_to_keep)\n",
    "        return data\n",
    "\n",
    "    def preprocess_static_cb_0(self, data):\n",
    "        return self.preprocess_static(data)\n",
    "\n",
    "    def merge_data(self, data_base, data_static, data_static_cb_0):\n",
    "        merged_data = data_base.join(data_static, on='case_id', how='left')\n",
    "        merged_data = merged_data.join(data_static_cb_0, on='case_id', how='left')\n",
    "        for col in ['education_1103M', 'maritalst_385M']:\n",
    "            if merged_data[col].dtype != pl.Categorical:\n",
    "                merged_data = merged_data.with_columns(merged_data[col].cast(pl.Categorical))\n",
    "        dummies = merged_data[['education_1103M', 'maritalst_385M']].to_dummies()\n",
    "        merged_data = merged_data.drop(['education_1103M', 'maritalst_385M'])\n",
    "        merged_data = pl.concat([merged_data, dummies], how='horizontal')\n",
    "        return merged_data\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        data_base = self.load_data(self.base_path)\n",
    "        data_static = pl.concat([self.load_data(p) for p in self.static_paths], how='vertical')\n",
    "        data_static_cb_0 = self.load_data(self.static_cb_0_path)\n",
    "        data_base = self.preprocess_base(data_base)\n",
    "        data_static = self.preprocess_static(data_static)\n",
    "        data_static_cb_0 = self.preprocess_static_cb_0(data_static_cb_0)\n",
    "        Depth_0 = self.merge_data(data_base, data_static, data_static_cb_0)\n",
    "        return Depth_0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_base.parquet\"\n",
    "    static_paths = [\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_0.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_1.parquet\",\n",
    "        \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_0_2.parquet\"\n",
    "    ]\n",
    "    static_cb_0_path = \"C:/Users/afise/.git/CreditRiskModel/Data/parquet_files/test/test_static_cb_0.parquet\"\n",
    "\n",
    "    pipeline = DataPipeline_Depth_0(base_path, static_paths, static_cb_0_path)\n",
    "    Depth_0 = pipeline.execute_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
